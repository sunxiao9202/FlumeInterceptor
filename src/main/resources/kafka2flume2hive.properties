a4.sources=s1
a4.sinks=k1
a4.channels=c1

#kafka为souce的配置
a4.sources.s1.type = org.apache.flume.source.kafka.KafkaSource
a4.sources.s1.batchSize = 5000
a4.sources.s1.batchDurationMillis = 2000
a4.sources.s1.kafka.bootstrap.servers =canal01:9092,canal02:9092,canal03:9092
a4.sources.s1.kafka.topics=BD_CC_STUDIED_LESSON_LOG
a4.sources.s1.kafka.consumer.group.id =BD_STUDIED_LOG_FLUME
#a4.sources.s1.kafka.consumer.value.deserializer= com.alibaba.otter.canal.client.kafka.MessageDeserializer
#a4.sources.s1.kafka.consumer.auto.offset.reset=latest
#a4.sources.s1.timeout.ms=1000

a4.sources.s1.interceptors=i1
a4.sources.s1.interceptors.i1.type=com.zhihuishu.flume.interceptor.StudytLogInterceptor$Builder
a4.sources.r1.selector.type=multiplexing
a4.sources.r1.selector.header=studyLog
a4.sources.r1.selector.mapping.successStudyLog=c1


# channels
# batchsize <=transactionCapacity<=capacity
a4.channels.c1.type=file
a4.channels.c1.checkpointDir=/flume/flumeFlu/checkPoint/successStudyLog
a4.channels.c1.useDualCheckpoints=true
a4.channels.c1.backupCheckpointDir=/flume/flumeFlu/backupCheckPoint/backupSuccessStudyLog
a4.channels.c1.dataDirs=/flume/flumeFlu/data/successStudyLog
a4.channels.c1.maxFileSize=2146435071
a4.channels.c1.transactionCapacity=200000
a4.channels.c1.capacity=100000000
a4.channels.c1.keep-alive=10

#hive为sink的配置
#a4.sinks.k1.type=hive
#a4.sinks.k1.hive.metastore = thrift://hadoop01:9083
#a4.sinks.k1.hive.database = ods
#a4.sinks.k1.hive.table = cc_studied_lesson_log
#a4.sinks.k1.hive.partition = %Y%m%d
#a4.sinks.k1.autoCreatePartitions = true
#a4.sinks.k1.batchSize = 15000
#a4.sinks.k1.useLocalTimeStamp = false
#a4.sinks.k1.round = true
#a4.sinks.k1.roundValue = 10
#a4.sinks.k1.roundUnit = second
#a4.sinks.k1.serializer = DELIMITED
#a4.sinks.k1.serializer.delimiter = ","
#a4.sinks.k1.serializer.serdeSeparator = ','
#a4.sinks.k1.serializer.fieldnames =id,user_id,recruit_id,lesson_id,lesson_video_id,video_id,play_times,study_total_time,learn_time,sourse_type,create_time

# hdfs为sink的配置
## sink
a4.sinks.k1.type=hdfs
a4.sinks.k1.hdfs.path=/hive/db/ods.db/cc_studied_lesson_log/partition_time=%Y%m%d
a4.sinks.k1.hdfs.filePrefix=cc_studied_lesson_log_

## 开启目录定时回滚,每10min回滚一次
#a4.sinks.k1.hdfs.round=true
#a4.sinks.k1.hdfs.roundValue=10
#a4.sinks.k1.hdfs.roundUnit=min


## 开启日志文件定时回滚,不要产生大量小文件
# 根据时间30min(1800)回滚文件
a4.sinks.k1.hdfs.rollInterval= 1800
# 当临时文件达到96M(100663296)时,文件回滚;为0,不根据文件大小回滚
a4.sinks.k1.hdfs.rollSize= 0
# 不根据event数量回滚文件
a4.sinks.k1.hdfs.rollCount= 0
# 10min(600)内临时文件没有数据写入,该文件回滚;为0,never roll based on file size
a4.sinks.k1.hdfs.idleTimeout = 0

## 控制输出文件是原生文件。
#a2.sinks.k3.hdfs.fileType=DataStream  
#a2.sinks.k3.hdfs.writeFormat=Text

## 控制输出文件是lzo类型
a4.sinks.k1.hdfs.fileType=CompressedStream  
a4.sinks.k1.hdfs.writeFormat=Text
a4.sinks.k1.hdfs.codeC=lzo

a4.sinks.k1.hdfs.minBlockReplicas=1

#指定timezone
#a1.sinks.k1.hdfs.useLocalTimeStamp = true
#a1.sinks.k1.hdfs.timeZone = Asia/Shanghai

#三者之间的关系
a4.sources.s1.channels = c1
a4.sinks.k1.channel = c1
